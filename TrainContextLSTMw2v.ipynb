{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import string\n",
    "import collections\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Bidirectional, Dropout, Dense, Activation\n",
    "from keras.optimizers import RMSprop, Adadelta, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import callbacks\n",
    "from keras.utils import multi_gpu_model\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import csv\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = string.punctuation.replace('.','')\n",
    "punct = str.maketrans('','', puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('iotr.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysents = pd.read_pickle(\"lor_sents.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullstoper(s):\n",
    "    return s[:-1] + [s[-1]+'.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysents['FullSents'] = mysents.Sentences.apply(fullstoper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contexts = set(tuple(x) for x in mysents.Context.values)\n",
    "#clusters = {idx:list(x) for idx,x in enumerate(contexts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_width = [len(x) for x in mysents.FullSents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_sents = [x for y in mysents.FullSents for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "breakpoint = 0\n",
    "acc = 0\n",
    "sequence_ip, context_ip, target_op = [],[],[]\n",
    "\n",
    "for position in range(len(continuous_sents) - seq_len):\n",
    "    sequence_ip.append(continuous_sents[position : position + seq_len])\n",
    "    target_op.append(continuous_sents[position + seq_len])\n",
    "    if position > sents_width[breakpoint] + acc - seq_len/2:\n",
    "        acc += sents_width[breakpoint]\n",
    "        breakpoint += 1\n",
    "    #print(\"position \", position, \" : \", position +seq_len, \"breakpiont \",breakpoint)\n",
    "    context_ip.append(mysents.Context[breakpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lordf = pd.DataFrame({'Sequence_ip':sequence_ip, 'Context_ip':context_ip, 'Target_op':target_op})\n",
    "lordf_100k = lordf[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocab to id and vice versa for the words in the sentences\n",
    "sentvocab = collections.Counter([x for y in lordf_100k['Sequence_ip'] for x in y])\n",
    "ctxvocab = collections.Counter([x for y in lordf_100k['Context_ip'] for x in y])\n",
    "vocab = sorted(sentvocab + ctxvocab)\n",
    "vocab2idx = {v:idx for idx,v in enumerate(vocab)}\n",
    "idx2vocab = {idx:v for idx,v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_arr = np.zeros((len(lordf_100k), seq_len+1, len(vocab)), dtype=bool)\n",
    "target_arr = np.zeros((len(lordf_100k), len(vocab)), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_idx,(x,y,z) in enumerate(zip(lordf_100k['Context_ip'], lordf_100k['Sequence_ip'], lordf_100k['Target_op'])):\n",
    "    sequence_arr[s_idx][0][[vocab2idx[val] for val in x]] = 1\n",
    "    target_arr[s_idx][vocab2idx[z]] = 1\n",
    "    for w_idx,word in enumerate(y):\n",
    "        sequence_arr[s_idx][w_idx+1][vocab2idx[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(256, activation='relu'), input_shape=(seq_len+1, len(vocab))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(vocab)))\n",
    "model.add(Activation('softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rms = RMSprop()\n",
    "opt_ada = Adadelta()\n",
    "opt_adam = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../../data/contextlstmv2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../../data/contextlstmv2_100k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=4)\n",
    "parallel_model.compile(optimizer=opt_ada, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.2, min_lr=0)\n",
    "batch_size = 128\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from ../w2v_wikimodel.hd5\n",
      "INFO:gensim.utils:loading wv recursively from ../w2v_wikimodel.hd5.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from ../w2v_wikimodel.hd5.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute vectors_norm to None\n",
      "INFO:gensim.utils:loading vocabulary recursively from ../w2v_wikimodel.hd5.vocabulary.* with mmap=None\n",
      "INFO:gensim.utils:loading trainables recursively from ../w2v_wikimodel.hd5.trainables.* with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from ../w2v_wikimodel.hd5.trainables.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:loaded ../w2v_wikimodel.hd5\n"
     ]
    }
   ],
   "source": [
    "embeddingmodel = gensim.models.word2vec.Word2Vec.load('../w2v_wikimodel.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the end word with full-stop is an alphabet\n",
    "def clean(xx):\n",
    "    return ' '.join(x for x in xx.split() if x.strip('.').isalpha())\n",
    "\n",
    "# remove all punctuations except full-stop for word endings\n",
    "cdata = clean(data.translate(punct))\n",
    "\n",
    "# Pre processing for word2vec contexts\n",
    "cdata_w2v = remove_stopwords(cdata.lower())\n",
    "sents_w2v = [s.lower().split() for s in cdata_w2v.split('.') if len(s.split())>5]\n",
    "vcab = sorted(set([w for s in sents_w2v for w in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permit the vocabulary from LOR dataset, only if it is present in wiki model\n",
    "common_vocab = [x for x in vcab if x in embeddingmodel.wv.vocab]\n",
    "vecs = {k:embeddingmodel.wv.get_vector(k) for k in common_vocab}\n",
    "wvvocab = list(vecs.keys())\n",
    "wvvecs = list(vecs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation.\n",
    "vecs = {k:embeddingmodel.wv.get_vector(k) for k in embeddingmodel.wv.vocab}\n",
    "wvvocab = list(vecs.keys())\n",
    "wvvecs = list(vecs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=100, random_state=0)\n",
    "km.fit(wvvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfidf_importance import get_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf. . .  {'woods': 0.22026683847554496, 'way': 0.15791757241453094, 'town': 0.24971313860547795, 'sword': 0.1975088672730664, 'spur': 0.2965002975303856, 'road': 0.1714632547197942, 'meantime': 0.2941718018418234, 'master': 0.18429834087460925, 'hobbits': 0.17046968232943605, 'great': 0.1427824920447543, 'goblins': 0.2166857650050281, 'goblin': 0.2556262417583239, 'elf': 0.2431604117397801, 'dwarves': 0.3716129345022729, 'deal': 0.2260195589358893, 'dark': 0.15568981456220948, 'chief': 0.23152211392340585, 'characters': 0.31538721905394335}\n",
      "Tfidf. . .  {'staff': 0.3184685386863208, 'others': 0.2456691006727009, 'line': 0.2933623370134616, 'laughter': 0.6838470572398085, 'floor': 0.2944814423950979, 'dwarves': 0.24335175977273024, 'door': 0.24932210593208687, 'below': 0.2764331980101654}\n",
      "Tfidf. . .  {'way': 0.4097664555307104, 'guards': 0.6322530436922931, 'end': 0.44709663973009917, 'dwarves': 0.48213290222270005}\n",
      "Tfidf. . .  {'pull': 0.6975436055234616, 'jug': 0.7165423353810502}\n",
      "Tfidf. . .  {'words': 0.4260874539269901, 'horses': 0.483416220822328, 'cave': 0.5617021389673267, 'birds': 0.5188920370668675}\n",
      "Tfidf. . .  {'way': 0.21863203759026562, 'track': 0.37488414739855896, 'town': 0.3457201847243722, 'time': 0.21690526132684596, 'share': 0.35698861806080745, 'rate': 0.3267745985020897, 'hours': 0.3262216267832866, 'fact': 0.3267745985020897, 'burglar': 0.38531065588678615, 'bilbo': 0.22181651979249617}\n",
      "Tfidf. . .  {'wood': 0.6081525784357616, 'hobbit': 0.5645776926336555, 'heart': 0.5580344705503345}\n",
      "Tfidf. . .  {'mountain': 0.43455464270464245, 'dwarves': 0.4019469278214755, 'direction': 0.5389011722068446, 'deal': 0.4889381337790328, 'bilbo': 0.3465919180383229}\n",
      "Tfidf. . .  {'valley': 0.26604240638918664, 'story': 0.30292893541384464, 'mountain': 0.2591176344175272, 'great': 0.184177011210575, 'eagle': 0.36084758873755757, 'dwarves': 0.7190226051913305, 'chief': 0.2986434146155788}\n"
     ]
    }
   ],
   "source": [
    "tfidfs = get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_closeness(context_word, sentence):\n",
    "    cumulative = 0\n",
    "    for word,tfidf in sentence.items():\n",
    "        cumulative += tfidf*embeddingmodel.wv.similarity(context_word, word)\n",
    "    return cumulative/sum(sentence.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gandalf .. 0.07034807089068781\n",
      "ring .. 0.09351642233281396\n",
      "war .. 0.0404397225474678\n",
      "friends .. -0.0005761456848445369\n",
      "snake .. 0.19579710321674806\n",
      "book .. 0.03020669099127537\n",
      "home .. 0.09126108632376154\n",
      "king .. 0.046763618660544105\n",
      "hobbit .. 0.0589704222870908\n"
     ]
    }
   ],
   "source": [
    "for sent in tfidfs:\n",
    "    print(sent[0], \"..\", evaluate_closeness(sent[0], sent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends', {'pull': 0.6975436055234616, 'jug': 0.7165423353810502}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclusters = km.predict(wvvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_clustercenters(num):\n",
    "    return [x[0] for x in embeddingmodel.wv.most_similar(positive=[km.cluster_centers_[num]], topn=5)]\n",
    "\n",
    "clusterlookup = {k:get_words_from_clustercenters(k) for k in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2word(arr):\n",
    "    return idx2vocab[arr.argmax()]\n",
    "\n",
    "def word2onehot(word):\n",
    "    vidx = vocab2idx[word]\n",
    "    varr = np.zeros((1, len(vocab)), dtype=bool)\n",
    "    varr[0, vidx] = 1\n",
    "    return varr\n",
    "\n",
    "def context2onehot(topic):\n",
    "    topicvec = embeddingmodel.wv.get_vector(topic)\n",
    "    clustervec = km.predict(topicvec.reshape(1,-1))[0]\n",
    "    words = clusterlookup[clustervec]\n",
    "    varr = np.zeros((1, len(vocab)), dtype=bool)\n",
    "    varr[0][[vocab2idx[val] for val in words]] = 1\n",
    "    return varr\n",
    "\n",
    "def prob2onehot(prob):\n",
    "    foo = np.zeros((1, len(vocab)), dtype=bool)\n",
    "    foo[0, prob.argmax()] = 1\n",
    "    return foo\n",
    "\n",
    "def headstart():\n",
    "    hswords = []\n",
    "    for w in 'hobbits lived in the woods and an elf'.split():\n",
    "        hswords.append(word2onehot(w))\n",
    "    return np.array(hswords).transpose(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hints = 'hobbits gollum adventure king ring war friends war book home'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, word_limit, context_words):\n",
    "    context_idx = 0\n",
    "    textcum = []\n",
    "    text_generated = []\n",
    "    input_arr = headstart()\n",
    "    context_arr = context2onehot(context_words.pop(0))\n",
    "    sequence_arr = np.concatenate((context_arr, input_arr[0]))[np.newaxis,:]\n",
    "    text_generated.extend([x for y in sequence_arr for x in y])\n",
    "    for idx in range(word_limit):\n",
    "        if len(context_words) > 0:\n",
    "            predicted_arr = prob2onehot(model.predict(sequence_arr))\n",
    "            text_generated.append(predicted_arr)\n",
    "            if '.' in onehot2word(predicted_arr):\n",
    "                context_arr = context2onehot(context_words.pop(0))\n",
    "            sequence_arr = np.concatenate((context_arr, sequence_arr[0, 2:, :], predicted_arr)).reshape(sequence_arr.shape)\n",
    "            print(onehot2word(predicted_arr))\n",
    "    for w in text_generated:\n",
    "        textcum.append(onehot2word(w))\n",
    "    return ' '.join(textcum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel_model.fit(sequence_arr, target_arr, batch_size=batch_size, epochs=20, callbacks=[lr_reducer], validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hints = 'Hobbits gollum adventure king ring war friends war book home'.split()\n",
    "\n",
    "for ep in range(400):\n",
    "    parallel_model.fit(sequence_arr, target_arr, batch_size=batch_size, epochs=3, callbacks=[lr_reducer], validation_split=0.10)\n",
    "    parallel_model.save('../../data/contextlstmv2_100k.h5')\n",
    "    #print(gen_text(parallel_model, 300, hints))\n",
    "    print(\"Epoch\", ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text(model, 300, hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hints = 'hobbits gollum adventure king ring war friends war book home'.split()\n",
    "gen_text(model, 300, hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequence_arr, target_arr, batch_size=batch_size, epochs=20, callbacks=[lr_reducer], validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
